{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 \n",
    "## Prueba de instalación NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/inputtransformer2.py:481: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  warnings.warn(\"`make_tokens_by_line` received a list of lines which do not have lineending markers ('\\\\n', '\\\\r', '\\\\r\\\\n', '\\\\x0b', '\\\\x0c'), behavior will be unspecified\")\n"
     ]
    }
   ],
   "source": [
    ">>> text = 'Ironhack is a Global Tech School ranked num 2 worldwide.  ",
    " ",
    "Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do. This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course. We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack is a Global Tech School ranked num 2 worldwide.',\n",
       " 'Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.',\n",
       " 'This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course.',\n",
       " 'We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Global',\n",
       " 'Tech',\n",
       " 'School',\n",
       " 'ranked',\n",
       " 'num',\n",
       " '2',\n",
       " 'worldwide',\n",
       " '.',\n",
       " 'Our',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'help',\n",
       " 'people',\n",
       " 'transform',\n",
       " 'their',\n",
       " 'careers',\n",
       " 'and',\n",
       " 'join',\n",
       " 'a',\n",
       " 'thriving',\n",
       " 'community',\n",
       " 'of',\n",
       " 'tech',\n",
       " 'professionals',\n",
       " 'that',\n",
       " 'love',\n",
       " 'what',\n",
       " 'they',\n",
       " 'do',\n",
       " '.',\n",
       " 'This',\n",
       " 'ideology',\n",
       " 'is',\n",
       " 'reflected',\n",
       " 'in',\n",
       " 'our',\n",
       " 'teaching',\n",
       " 'practices',\n",
       " ',',\n",
       " 'which',\n",
       " 'consist',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nine-weeks',\n",
       " 'immersive',\n",
       " 'programming',\n",
       " ',',\n",
       " 'UX/UI',\n",
       " 'design',\n",
       " 'or',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'course',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'one-week',\n",
       " 'hiring',\n",
       " 'fair',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'helping',\n",
       " 'our',\n",
       " 'students',\n",
       " 'change',\n",
       " 'their',\n",
       " 'career',\n",
       " 'and',\n",
       " 'get',\n",
       " 'a',\n",
       " 'job',\n",
       " 'straight',\n",
       " 'after',\n",
       " 'the',\n",
       " 'course',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'present',\n",
       " 'in',\n",
       " '8',\n",
       " 'countries',\n",
       " 'and',\n",
       " 'have',\n",
       " 'campuses',\n",
       " 'in',\n",
       " '9',\n",
       " 'locations',\n",
       " '-',\n",
       " 'Madrid',\n",
       " ',',\n",
       " 'Barcelona',\n",
       " ',',\n",
       " 'Miami',\n",
       " ',',\n",
       " 'Paris',\n",
       " ',',\n",
       " 'Mexico',\n",
       " 'City',\n",
       " ',',\n",
       " 'Berlin',\n",
       " ',',\n",
       " 'Amsterdam',\n",
       " ',',\n",
       " 'Sao',\n",
       " 'Paulo',\n",
       " 'and',\n",
       " 'Lisbon',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_url_1 = 'https?\\:\\/{2}[a-z]*\\.?[a-zA-Z0-9@:%._\\+~#=]*\\.[a-zA-Z0-9()]*\\b?[-a-zA-Z0-9()@:%_\\+.~#?&//=]*' # con http \n",
    "#path_url_2 = '[-a-zA-Z0-9@:%._\\+~#=]*\\.[a-zA-Z0-9()]*\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)' # sin http\n",
    "path_num = '[0-9]'\n",
    "path_car = '\\W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de limpieza de strings\n",
    "\n",
    "def clean_up(s):\n",
    "    \"\"\"\n",
    "    Cleans up numbers, URLs, and special characters from a string.\n",
    "\n",
    "    Args:\n",
    "        s: The string to be cleaned up.\n",
    "\n",
    "    Returns:\n",
    "        A string that has been cleaned up.\n",
    "    \"\"\"\n",
    "    s = re.sub(path_url_1, \" \", s).lower()\n",
    "    s = re.sub(path_num, \" \",s)\n",
    "    s = re.sub(path_car,\" \",s)\n",
    "    return s\n",
    "\n",
    "# Prueba\n",
    "#prueba = '''@Ironhack's-#Q website 776-is http://ironhack.com [(2018)]\")'''\n",
    "#clean_up(prueba)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "def tokenize(s):\n",
    "    return word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "# Stemming & Lemmatizing\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def stem_and_lemmatize(word_list):\n",
    "    porter, lemma = PorterStemmer(),WordNetLemmatizer()  # inicializamos el stemmer y el lemmatizer\n",
    "    lemm_stemm = [porter.stem(lemma.lemmatize(i)) for i in word_list] # Stemming -> Lemmatization\n",
    "    return lemm_stemm\n",
    "\n",
    "# PRUEBA\n",
    "#word_list = ['hours',\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "#stem_and_lemmatize(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "# https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(lst_stem): # Si la letra no esta en la lista de stopwords, no se agrega a la nueva lista.\n",
    "    filtered_list= []\n",
    "    for i in lst_stem:\n",
    "        if i not in stop_words:\n",
    "            filtered_list.append(i)\n",
    "            \n",
    "    return filtered_list\n",
    "\n",
    "# Prueba\n",
    "#word_list = ['a','hours',\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "#remove_stopwords(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el csv\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "x = zipfile.ZipFile('Sentiment140.csv.zip')\n",
    "df = pd.read_csv(x.open('Sentiment140.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "1       0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "2       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!   \n",
       "1      sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...  \n",
       "2       sammydearr  @TiannaChaos i know  just family drama. its la...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agarramos un muestra del df de 20k\n",
    "sample = df.sample(n=20000, random_state = 42).reset_index(drop = True) \n",
    "sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos las funciones de limpieza del challenge 2 a cada row de la columna 'text'\n",
    "\n",
    "sample['text_processed'] = sample['text'].apply(clean_up).apply(tokenize).apply(stem_and_lemmatize).apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "      <td>[chrishasboob, ahhh, hope, ok]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "      <td>[misstoriblack, cool, tweet, app, razr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "      <td>[tiannachao, know, famili, drama, lame, hey, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "      <td>[school, email, open, geographi, stuff, revis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "      <td>[upper, airway, problem]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "1       0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "2       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "3       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
       "4       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
       "1      sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...   \n",
       "2       sammydearr  @TiannaChaos i know  just family drama. its la...   \n",
       "3      Lamb_Leanne  School email won't open  and I have geography ...   \n",
       "4      yogicerdito                             upper airways problem    \n",
       "\n",
       "                                      text_processed  \n",
       "0                     [chrishasboob, ahhh, hope, ok]  \n",
       "1            [misstoriblack, cool, tweet, app, razr]  \n",
       "2  [tiannachao, know, famili, drama, lame, hey, n...  \n",
       "3  [school, email, open, geographi, stuff, revis,...  \n",
       "4                           [upper, airway, problem]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queremos conocer las palabras más frecuentes en el sample de tweets que tenemos.\n",
    "\n",
    "from nltk import FreqDist \n",
    "lst_words = []\n",
    "for i in sample['text_processed']:\n",
    "    lst_words = lst_words+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'wa', 'get', 'day', 'good', 'thi', 'work', 'like', 'love', 'quot']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = FreqDist(lst_words)\n",
    "most_common = freq.most_common(5000)\n",
    "keys_common = [i[0] for i in most_common]  \n",
    "keys_common[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def find_features(tweets):\n",
    "    features = {}\n",
    "    for i in keys_common:\n",
    "        features[i] = (i in tweets)\n",
    "        \n",
    "    s = SentimentIntensityAnalyzer().polarity_scores(\" \".join(tweets))\n",
    "    if s['pos'] > 0.3:\n",
    "        s = True\n",
    "    else:\n",
    "        s = False\n",
    "        \n",
    "    return (features, s)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [find_features(i) for i in sample.text_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos la base en  train y test\n",
    "train = features[:10000]\n",
    "test = features[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos los datos de train\n",
    "classifier = NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 perfect = True             True : False  =     25.7 : 1.0\n",
      "                   laugh = True             True : False  =     22.4 : 1.0\n",
      "                  hahaha = True             True : False  =     19.8 : 1.0\n",
      "                      xd = True             True : False  =     18.9 : 1.0\n",
      "                   relax = True             True : False  =     17.2 : 1.0\n",
      "                  talent = True             True : False  =     16.9 : 1.0\n",
      "                    luck = True             True : False  =     16.1 : 1.0\n",
      "                 comfort = True             True : False  =     12.5 : 1.0\n",
      "                    hero = True             True : False  =     12.5 : 1.0\n",
      "                   grand = True             True : False  =     12.5 : 1.0\n",
      "                    love = True             True : False  =     12.1 : 1.0\n",
      "                     win = True             True : False  =     12.0 : 1.0\n",
      "                   great = True             True : False  =     11.2 : 1.0\n",
      "                    xoxo = True             True : False  =     11.0 : 1.0\n",
      "                  accept = True             True : False  =     11.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8432"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se obtuvo una exactitud del 84% entonces se puede decir que el modelo está haciendo un buen trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
