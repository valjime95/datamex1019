b'<!DOCTYPE html>\n<html class="client-nojs" lang="en" dir="ltr">\n<head>\n<meta charset="UTF-8"/>\n<title>Multilayer perceptron - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Multilayer_perceptron","wgTitle":"Multilayer perceptron","wgCurRevisionId":923044913,"wgRevisionId":923044913,"wgArticleId":2266644,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Classification algorithms","Artificial neural networks"],"wgBreakFrames":!1,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Multilayer_perceptron","wgRelevantArticleId":2266644,"wgRequestId":"XbaLDApAAEAAAAK8nk0AAABN","wgCSPNonce":!1,\n"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q2991667","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready",\n"mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP",\n"ext.centralNotice.startUp","skins.vector.js"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta name="ResourceLoaderDynamicStyles" content=""/>\n<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>\n<meta name="generator" content="MediaWiki 1.35.0-wmf.3"/>\n<meta name="referrer" content="origin"/>\n<meta name="referrer" content="origin-when-crossorigin"/>\n<meta name="referrer" content="origin-when-cross-origin"/>\n<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png"/>\n<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Multilayer_perceptron"/>\n<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Multilayer_perceptron&amp;action=edit"/>\n<link rel="edit" title="Edit this page" href="/w/index.php?title=Multilayer_perceptron&amp;action=edit"/>\n<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>\n<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>\n<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>\n<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>\n<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>\n<link rel="canonical" href="https://en.wikipedia.org/wiki/Multilayer_perceptron"/>\n<link rel="dns-prefetch" href="//login.wikimedia.org"/>\n<link rel="dns-prefetch" href="//meta.wikimedia.org" />\n<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Multilayer_perceptron rootpage-Multilayer_perceptron skin-vector action-view">\n<div id="mw-page-base" class="noprint"></div>\n<div id="mw-head-base" class="noprint"></div>\n<div id="content" class="mw-body" role="main">\n\t<a id="top"></a>\n\t<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>\n\t<div class="mw-indicators mw-body-content">\n</div>\n\n\t<h1 id="firstHeading" class="firstHeading" lang="en">Multilayer perceptron</h1>\n\t\n\t<div id="bodyContent" class="mw-body-content">\n\t\t<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>\n\t\t<div id="contentSub"></div>\n\t\t\n\t\t\n\t\t\n\t\t<div id="jump-to-nav"></div>\n\t\t<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n\t\t<a class="mw-jump-link" href="#p-search">Jump to search</a>\n\t\t<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div role="note" class="hatnote navigation-not-searchable">"MLP" is not to be confused with "NLP", which refers to <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>.</div> \n<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br /><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" decoding="async" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></td></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a href="/wiki/CURE_data_clustering_algorithm" class="mw-redirect" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_networks" class="mw-redirect" title="Artificial neural networks">Artificial neural networks</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a class="mw-selflink selflink">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Bias%E2%80%93variance_dilemma" class="mw-redirect" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p>A <b>multilayer perceptron</b> (MLP) is a class of <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward</a> <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> (ANN). The term MLP is used ambiguously, sometimes loosely to refer to <i>any</i> feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of <a href="/wiki/Perceptron" title="Perceptron">perceptrons</a> (with threshold activation); see <a href="#Terminology">\xc2\xa7&#160;Terminology</a>. Multilayer perceptrons are sometimes colloquially referred to as "vanilla" neural networks, especially when they have a single hidden layer.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup>\n</p><p>An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear <a href="/wiki/Activation_function" title="Activation function">activation function</a>. MLP utilizes a <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> technique called <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> for training.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup><sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> Its multiple layers and non-linear activation distinguish MLP from a linear <a href="/wiki/Perceptron" title="Perceptron">perceptron</a>. It can distinguish data that is not <a href="/wiki/Linear_separability" title="Linear separability">linearly separable</a>.<sup id="cite_ref-Cybenko1989_4-0" class="reference"><a href="#cite_note-Cybenko1989-4">&#91;4&#93;</a></sup>\n</p>\n<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Theory"><span class="tocnumber">1</span> <span class="toctext">Theory</span></a>\n<ul>\n<li class="toclevel-2 tocsection-2"><a href="#Activation_function"><span class="tocnumber">1.1</span> <span class="toctext">Activation function</span></a></li>\n<li class="toclevel-2 tocsection-3"><a href="#Layers"><span class="tocnumber">1.2</span> <span class="toctext">Layers</span></a></li>\n<li class="toclevel-2 tocsection-4"><a href="#Learning"><span class="tocnumber">1.3</span> <span class="toctext">Learning</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-5"><a href="#Terminology"><span class="tocnumber">2</span> <span class="toctext">Terminology</span></a></li>\n<li class="toclevel-1 tocsection-6"><a href="#Applications"><span class="tocnumber">3</span> <span class="toctext">Applications</span></a></li>\n<li class="toclevel-1 tocsection-7"><a href="#References"><span class="tocnumber">4</span> <span class="toctext">References</span></a></li>\n<li class="toclevel-1 tocsection-8"><a href="#External_links"><span class="tocnumber">5</span> <span class="toctext">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class="mw-headline" id="Theory">Theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=1" title="Edit section: Theory">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<h3><span class="mw-headline" id="Activation_function">Activation function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=2" title="Edit section: Activation function">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>If a multilayer perceptron has a linear <a href="/wiki/Activation_function" title="Activation function">activation function</a> in all neurons, that is, a linear function that maps the <a href="/wiki/Synaptic_weight" title="Synaptic weight">weighted inputs</a> to the output of each neuron, then <a href="/wiki/Linear_algebra" title="Linear algebra">linear algebra</a> shows that any number of layers can be reduced to a two-layer input-output model. In MLPs some neurons use a <i>nonlinear</i> activation function that was developed to model the frequency of <a href="/wiki/Action_potentials" class="mw-redirect" title="Action potentials">action potentials</a>, or firing, of biological neurons.\n</p><p>The two historically common activation functions are both <a href="/wiki/Sigmoids" class="mw-redirect" title="Sigmoids">sigmoids</a>, and are described by\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>y</mi>\n        <mo stretchy="false">(</mo>\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mi>tanh</mi>\n        <mo>&#x2061;<!-- \xe2\x81\xa1 --></mo>\n        <mo stretchy="false">(</mo>\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">)</mo>\n        <mtext>&#xA0;</mtext>\n        <mtext>&#xA0;</mtext>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mtext>and</mtext>\n          </mrow>\n        </mrow>\n        <mtext>&#xA0;</mtext>\n        <mtext>&#xA0;</mtext>\n        <mi>y</mi>\n        <mo stretchy="false">(</mo>\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mo stretchy="false">(</mo>\n        <mn>1</mn>\n        <mo>+</mo>\n        <msup>\n          <mi>e</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n            <msub>\n              <mi>v</mi>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mi>i</mi>\n              </mrow>\n            </msub>\n          </mrow>\n        </msup>\n        <msup>\n          <mo stretchy="false">)</mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n            <mn>1</mn>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/167e8b5c38130ec92a2771bc384658772f387d02" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:42.601ex; height:3.176ex;" alt="y(v_i) = \\tanh(v_i) ~~ \\textrm{and} ~~ y(v_i) = (1+e^{-v_i})^{-1}"/></span>.</dd></dl>\n<p>In recent developments of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a> the <a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">rectifier linear unit (ReLU)</a> is more frequently used as one of the possible ways to overcome the numerical <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">problems</a> related to the sigmoids.\n</p><p>The first is a <a href="/wiki/Hyperbolic_tangent" class="mw-redirect" title="Hyperbolic tangent">hyperbolic tangent</a> that ranges from -1 to 1, while the other is the <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a>, which is similar in shape but ranges from 0 to 1. Here <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle y_{i}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>y</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle y_{i}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.939ex; height:2.009ex;" alt="y_{i}"/></span> is the output of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle i}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>i</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;" alt="i"/></span>th node (neuron) and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle v_{i}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle v_{i}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7dffe5726650f6daac54829972a94f38eb8ec127" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.927ex; height:2.009ex;" alt="v_{i}"/></span> is the weighted sum of the input connections. Alternative activation functions have been proposed, including the <a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">rectifier and softplus</a> functions. More specialized activation functions include <a href="/wiki/Radial_basis_functions" class="mw-redirect" title="Radial basis functions">radial basis functions</a> (used in <a href="/wiki/Radial_basis_network" class="mw-redirect" title="Radial basis network">radial basis networks,</a> another class of supervised neural network models).\n</p>\n<h3><span class="mw-headline" id="Layers">Layers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=3" title="Edit section: Layers">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The MLP consists of three or more layers (an input and an output layer with one or more <i>hidden layers</i>) of nonlinearly-activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle w_{ij}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>w</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n            <mi>j</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle w_{ij}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="w_{ij}"/></span> to every node in the following layer.\n</p>\n<h3><span class="mw-headline" id="Learning">Learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=4" title="Edit section: Learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, and is carried out through <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>, a generalization of the <a href="/wiki/Least_mean_squares_filter" title="Least mean squares filter">least mean squares algorithm</a> in the linear perceptron.\n</p><p>We can represent the degree of error in an output node <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle j}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>j</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="j"/></span> in the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle n}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>n</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle n}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;" alt="n"/></span>th data point (training example) by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>e</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <msub>\n          <mi>d</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <msub>\n          <mi>y</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ae245b407e23f310e793e20e6a88655b43f03c3" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:21.712ex; height:3.009ex;" alt="e_j(n)=d_j(n)-y_j(n)"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle d}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>d</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle d}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.216ex; height:2.176ex;" alt="d"/></span> is the target value and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle y}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>y</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle y}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;" alt="y"/></span> is the value produced by the perceptron. The node weights can then be adjusted based on corrections that minimize the error in the entire output, given by\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{j}e_{j}^{2}(n)}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">E</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mfrac>\n            <mn>1</mn>\n            <mn>2</mn>\n          </mfrac>\n        </mrow>\n        <munder>\n          <mo>&#x2211;<!-- \xe2\x88\x91 --></mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </munder>\n        <msubsup>\n          <mi>e</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msubsup>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{j}e_{j}^{2}(n)}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41454c8f3507f945e99dc7e18e8225d1bb0830de" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:19.083ex; height:6.676ex;" alt="\\mathcal{E}(n)=\\frac{1}{2}\\sum_j e_j^2(n)"/></span>.</dd></dl>\n<p>Using <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>, the change in each weight is\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi mathvariant="normal">&#x0394;<!-- \xce\x94 --></mi>\n        <msub>\n          <mi>w</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <mi>&#x03B7;<!-- \xce\xb7 --></mi>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mfrac>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi class="MJX-tex-caligraphic" mathvariant="script">E</mi>\n                </mrow>\n              </mrow>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <msub>\n                <mi>v</mi>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi>j</mi>\n                </mrow>\n              </msub>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n          </mfrac>\n        </mrow>\n        <msub>\n          <mi>y</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e775e1fd516ec50eaf45344d5429657686c6985c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:26.896ex; height:6.509ex;" alt="\\Delta w_{ji} (n) = -\\eta\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle y_{i}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>y</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle y_{i}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.939ex; height:2.009ex;" alt="y_{i}"/></span> is the output of the previous neuron and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\eta }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03B7;<!-- \xce\xb7 --></mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\eta }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="\\eta "/></span> is the <i><a href="/wiki/Learning_rate" title="Learning rate">learning rate</a></i>, which is selected to ensure that the weights quickly converge to a response, without oscillations.\n</p><p>The derivative to be calculated depends on the induced local field <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle v_{j}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle v_{j}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/73fffa4919c0d6268f6a8d9f38c04dd3296fd0a5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.037ex; height:2.343ex;" alt="v_{j}"/></span>, which itself varies. It is easy to prove that for an output node this derivative can be simplified to\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mfrac>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi class="MJX-tex-caligraphic" mathvariant="script">E</mi>\n                </mrow>\n              </mrow>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <msub>\n                <mi>v</mi>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi>j</mi>\n                </mrow>\n              </msub>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n          </mfrac>\n        </mrow>\n        <mo>=</mo>\n        <msub>\n          <mi>e</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <msup>\n          <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-variant" mathvariant="normal">&#x2032;<!-- \xe2\x80\xb2 --></mi>\n          </mrow>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/056be9bc7c738ade1a15914654576d0de972594b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:26.62ex; height:6.509ex;" alt="-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = e_j(n)\\phi^\\prime (v_j(n))"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\phi ^{\\prime }}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msup>\n          <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-variant" mathvariant="normal">&#x2032;<!-- \xe2\x80\xb2 --></mi>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\phi ^{\\prime }}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2a85fb8b93a9d888be883514370288e637780e04" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.07ex; height:2.843ex;" alt="\\phi^\\prime"/></span> is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mfrac>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi class="MJX-tex-caligraphic" mathvariant="script">E</mi>\n                </mrow>\n              </mrow>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <msub>\n                <mi>v</mi>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi>j</mi>\n                </mrow>\n              </msub>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n          </mfrac>\n        </mrow>\n        <mo>=</mo>\n        <msup>\n          <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-variant" mathvariant="normal">&#x2032;<!-- \xe2\x80\xb2 --></mi>\n          </mrow>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <msub>\n          <mi>v</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n        <munder>\n          <mo>&#x2211;<!-- \xe2\x88\x91 --></mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>k</mi>\n          </mrow>\n        </munder>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mfrac>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi class="MJX-tex-caligraphic" mathvariant="script">E</mi>\n                </mrow>\n              </mrow>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n            <mrow>\n              <mi mathvariant="normal">&#x2202;<!-- \xe2\x88\x82 --></mi>\n              <msub>\n                <mi>v</mi>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi>k</mi>\n                </mrow>\n              </msub>\n              <mo stretchy="false">(</mo>\n              <mi>n</mi>\n              <mo stretchy="false">)</mo>\n            </mrow>\n          </mfrac>\n        </mrow>\n        <msub>\n          <mi>w</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>k</mi>\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mi>n</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a57fb40387f833ae8d731f78c04138ad2ce6890b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:41.569ex; height:6.843ex;" alt="-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = \\phi^\\prime (v_j(n))\\sum_k -\\frac{\\partial\\mathcal{E}(n)}{\\partial v_k(n)} w_{kj}(n)"/></span>.</dd></dl>\n<p>This depends on the change in weights of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle k}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>k</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle k}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;" alt="k"/></span>th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup>\n</p><p><br />\n</p>\n<h2><span class="mw-headline" id="Terminology">Terminology</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=5" title="Edit section: Terminology">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The term "multilayer perceptron" does not refer to a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organized into layers. An alternative is "multilayer perceptron network". Moreover, MLP "perceptrons" are not perceptrons in the strictest possible sense. True perceptrons are formally a special case of artificial neurons that use a threshold activation function such as the <a href="/wiki/Heaviside_step_function" title="Heaviside step function">Heaviside step function</a>. MLP perceptrons can employ arbitrary activation functions. A true perceptron performs binary classification (either this or that), an MLP neuron is free to either perform classification or regression, depending upon its activation function.\n</p><p>The term "multilayer perceptron" later was applied without respect to nature of the nodes/layers, which can be composed of arbitrarily defined artificial neurons, and not perceptrons specifically. This interpretation avoids the loosening of the definition of "perceptron" to mean an artificial neuron in general.\n</p>\n<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=6" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>MLPs are useful in research for their ability to solve problems stochastically, which often allows approximate solutions for extremely <a href="/wiki/Computational_complexity_theory" title="Computational complexity theory">complex</a> problems like <a href="/wiki/Fitness_approximation" title="Fitness approximation">fitness approximation</a>.\n</p><p>MLPs are universal function approximators as shown by Cybenko\'s theorem,<sup id="cite_ref-Cybenko1989_4-1" class="reference"><a href="#cite_note-Cybenko1989-4">&#91;4&#93;</a></sup> so they can be used to create mathematical models by regression analysis. As <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> is a particular case of <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> when the response variable is <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a>, MLPs make good classifier algorithms.\n</p><p>MLPs were a popular machine learning solution in the 1980s, finding applications in diverse fields such as <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Image_recognition" class="mw-redirect" title="Image recognition">image recognition</a>, and <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a> software,<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> but thereafter faced strong competition from much simpler (and related<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup>) <a href="/wiki/Support_vector_machine" class="mw-redirect" title="Support vector machine">support vector machines</a>. Interest in backpropagation networks returned due to the successes of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>.\n</p>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=7" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">\n<ol class="references">\n<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">Hastie, Trevor. Tibshirani, Robert. Friedman, Jerome. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, New York, NY, 2009.</span>\n</li>\n<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">Rosenblatt, Frank. x. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Spartan Books, Washington DC, 1961</span>\n</li>\n<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams. "<a rel="nofollow" class="external text" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf">Learning Internal Representations by Error Propagation</a>". David E. Rumelhart, James L. McClelland, and the PDP research group. (editors), Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundation. MIT Press, 1986.</span>\n</li>\n<li id="cite_note-Cybenko1989-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-Cybenko1989_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Cybenko1989_4-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Cybenko, G. 1989. Approximation by superpositions of a sigmoidal function <i><a href="/wiki/Mathematics_of_Control,_Signals,_and_Systems" title="Mathematics of Control, Signals, and Systems">Mathematics of Control, Signals, and Systems</a></i>, 2(4), 303\xe2\x80\x93314.</span>\n</li>\n<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Simon_Haykin" title="Simon Haykin">Haykin, Simon</a> (1998). <i>Neural Networks: A Comprehensive Foundation</i> (2 ed.). Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-13-273350-1" title="Special:BookSources/0-13-273350-1"><bdi>0-13-273350-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neural+Networks%3A+A+Comprehensive+Foundation&amp;rft.edition=2&amp;rft.pub=Prentice+Hall&amp;rft.date=1998&amp;rft.isbn=0-13-273350-1&amp;rft.aulast=Haykin&amp;rft.aufirst=Simon&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultilayer+perceptron" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>\n</li>\n<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Neural networks. II. What are they and why is everybody so interested in them now?; Wasserman, P.D.; Schwartz, T.; Page(s): 10-15; IEEE Expert, 1988, Volume 3, Issue 1</span>\n</li>\n<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">R. Collobert and S. Bengio (2004). Links between Perceptrons, MLPs and SVMs. Proc. Int\'l Conf. on Machine Learning (ICML).</span>\n</li>\n</ol></div>\n<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit&amp;section=8" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/266396438_A_Gentle_Introduction_to_Backpropagation">A Gentle Introduction to Backpropagation - An intuitive tutorial by Shashi Sathyanarayana</a> This is an updated PDF version of a blog article that was previously linked here. This article contains pseudocode ("Training Wheels for Training Neural Networks") for implementing the algorithm.</li>\n<li><a rel="nofollow" class="external text" href="http://www.cs.waikato.ac.nz/ml/weka/">Weka: Open source data mining software with multilayer perceptron implementation</a>.</li>\n<li><a rel="nofollow" class="external text" href="http://neuroph.sourceforge.net/">Neuroph Studio documentation, implements this algorithm and a few others</a>.</li></ul>\n<!-- \nNewPP limit report\nParsed by mw1326\nCached time: 20191026001004\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.164 seconds\nReal time usage: 0.307 seconds\nPreprocessor visited node count: 525/1000000\nPreprocessor generated node count: 0/1500000\nPost\xe2\x80\x90expand include size: 28079/2097152 bytes\nTemplate argument size: 728/2097152 bytes\nHighest expansion depth: 11/40\nExpensive parser function count: 0/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 6353/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.050/10.000 seconds\nLua memory usage: 2.01 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  157.674      1 -total\n 51.20%   80.722      1 Template:Reflist\n 38.27%   60.346      1 Template:Cite_book\n 26.32%   41.503      1 Template:Machine_learning_bar\n 24.60%   38.794      1 Template:Sidebar_with_collapsible_lists\n 12.75%   20.098      1 Template:Longitem\n 11.05%   17.416      1 Template:Hatnote\n  9.83%   15.499      1 Template:Nobold\n  7.38%   11.638      1 Template:Slink\n  4.30%    6.781      1 Template:Small\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:2266644-0!canonical!math=5 and timestamp 20191026001004 and revision id 923044913\n -->\n</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>\n\t\t\n\t\t<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Multilayer_perceptron&amp;oldid=923044913">https://en.wikipedia.org/w/index.php?title=Multilayer_perceptron&amp;oldid=923044913</a>"</div>\n\t\t\n\t\t<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div></div>\n\t\t<div class="visualClear"></div>\n\t\t\n\t</div>\n</div>\n<div id=\'mw-data-after-content\'>\n\t<div class="read-more-container"></div>\n</div>\n\n\n\t\t<div id="mw-navigation">\n\t\t\t<h2>Navigation menu</h2>\n\t\t\t<div id="mw-head">\n\t\t\t\t\t\t\t\t\t<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">\n\t\t\t\t\t\t<h3 id="p-personal-label">Personal tools</h3>\n\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Multilayer+perceptron" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Multilayer+perceptron" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t<div id="left-navigation">\n\t\t\t\t\t\t\t\t\t\t<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">\n\t\t\t\t\t\t<h3 id="p-namespaces-label">Namespaces</h3>\n\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Multilayer_perceptron" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Multilayer_perceptron" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">\n\t\t\t\t\t\t\t\t\t\t\t\t<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />\n\t\t\t\t\t\t<h3 id="p-variants-label">\n\t\t\t\t\t\t\t<span>Variants</span>\n\t\t\t\t\t\t</h3>\n\t\t\t\t\t\t<ul class="menu">\n\t\t\t\t\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t<div id="right-navigation">\n\t\t\t\t\t\t\t\t\t\t<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">\n\t\t\t\t\t\t<h3 id="p-views-label">Views</h3>\n\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Multilayer_perceptron">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Multilayer_perceptron&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">\n\t\t\t\t\t\t<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />\n\t\t\t\t\t\t<h3 id="p-cactions-label"><span>More</span></h3>\n\t\t\t\t\t\t<ul class="menu">\n\t\t\t\t\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id="p-search" role="search">\n\t\t\t\t\t\t<h3>\n\t\t\t\t\t\t\t<label for="searchInput">Search</label>\n\t\t\t\t\t\t</h3>\n\t\t\t\t\t\t<form action="/w/index.php" id="searchform">\n\t\t\t\t\t\t\t<div id="simpleSearch">\n\t\t\t\t\t\t\t\t<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t</form>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t</div>\n\t\t\t<div id="mw-panel">\n\t\t\t\t<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>\n\t\t\t\t\t\t<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">\n\t\t\t<h3 id="p-navigation-label">Navigation</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content \xe2\x80\x93 the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">\n\t\t\t<h3 id="p-interaction-label">Interaction</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">\n\t\t\t<h3 id="p-tb-label">Tools</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Multilayer_perceptron" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Multilayer_perceptron" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Multilayer_perceptron&amp;oldid=923044913" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Multilayer_perceptron&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q2991667" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Multilayer_perceptron&amp;id=923044913" title="Information on how to cite this page">Cite this page</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">\n\t\t\t<h3 id="p-coll-print_export-label">Print/export</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Multilayer+perceptron">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Multilayer+perceptron&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Multilayer_perceptron&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">\n\t\t\t<h3 id="p-lang-label">Languages</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li class="interlanguage-link interwiki-bg"><a href="https://bg.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B5%D0%BD_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD" title="\xd0\x9c\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb5\xd0\xbd \xd0\xbf\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xe2\x80\x93 Bulgarian" lang="bg" hreflang="bg" class="interlanguage-link-target">\xd0\x91\xd1\x8a\xd0\xbb\xd0\xb3\xd0\xb0\xd1\x80\xd1\x81\xd0\xba\xd0\xb8</a></li><li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Perceptr%C3%B3_multicapa" title="Perceptr\xc3\xb3 multicapa \xe2\x80\x93 Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Catal\xc3\xa0</a></li><li class="interlanguage-link interwiki-da"><a href="https://da.wikipedia.org/wiki/Flerlags-perceptron" title="Flerlags-perceptron \xe2\x80\x93 Danish" lang="da" hreflang="da" class="interlanguage-link-target">Dansk</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Perzeptron#Mehrlagiges_Perzeptron" title="Perzeptron \xe2\x80\x93 German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Perceptr%C3%B3n_multicapa" title="Perceptr\xc3\xb3n multicapa \xe2\x80\x93 Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Espa\xc3\xb1ol</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D9%BE%D8%B1%D8%B3%D9%BE%D8%AA%D8%B1%D9%88%D9%86_%DA%86%D9%86%D8%AF%D9%84%D8%A7%DB%8C%D9%87" title="\xd9\xbe\xd8\xb1\xd8\xb3\xd9\xbe\xd8\xaa\xd8\xb1\xd9\x88\xd9\x86 \xda\x86\xd9\x86\xd8\xaf\xd9\x84\xd8\xa7\xdb\x8c\xd9\x87 \xe2\x80\x93 Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Perceptron_multicouche" title="Perceptron multicouche \xe2\x80\x93 French" lang="fr" hreflang="fr" class="interlanguage-link-target">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Percettrone_multistrato" title="Percettrone multistrato \xe2\x80\x93 Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3" title="\xe5\xa4\x9a\xe5\xb1\xa4\xe3\x83\x91\xe3\x83\xbc\xe3\x82\xbb\xe3\x83\x97\xe3\x83\x88\xe3\x83\xad\xe3\x83\xb3 \xe2\x80\x93 Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-pl"><a href="https://pl.wikipedia.org/wiki/Perceptron_wielowarstwowy" title="Perceptron wielowarstwowy \xe2\x80\x93 Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-pt"><a href="https://pt.wikipedia.org/wiki/Perceptron_multicamadas" title="Perceptron multicamadas \xe2\x80\x93 Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target">Portugu\xc3\xaas</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD_%D0%A0%D1%83%D0%BC%D0%B5%D0%BB%D1%8C%D1%85%D0%B0%D1%80%D1%82%D0%B0" title="\xd0\x9c\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbf\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xd0\xa0\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbb\xd1\x8c\xd1\x85\xd0\xb0\xd1\x80\xd1\x82\xd0\xb0 \xe2\x80\x93 Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%91%D0%B0%D0%B3%D0%B0%D1%82%D0%BE%D1%88%D0%B0%D1%80%D0%BE%D0%B2%D0%B8%D0%B9_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD_%D0%A0%D1%83%D0%BC%D0%B5%D0%BB%D1%8C%D1%85%D0%B0%D1%80%D1%82%D0%B0" title="\xd0\x91\xd0\xb0\xd0\xb3\xd0\xb0\xd1\x82\xd0\xbe\xd1\x88\xd0\xb0\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb8\xd0\xb9 \xd0\xbf\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xd0\xa0\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbb\xd1\x8c\xd1\x85\xd0\xb0\xd1\x80\xd1\x82\xd0\xb0 \xe2\x80\x93 Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8" title="\xe5\xa4\x9a\xe5\xb1\x82\xe6\x84\x9f\xe7\x9f\xa5\xe5\x99\xa8 \xe2\x80\x93 Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">\xe4\xb8\xad\xe6\x96\x87</a></li>\t\t\t\t</ul>\n\t\t\t\t<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q2991667#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>\t\t\t</div>\n\t\t</div>\n\t\t\t\t</div>\n\t\t</div>\n\t\t\t\t<div id="footer" role="contentinfo">\n\t\t\t\t\t\t<ul id="footer-info">\n\t\t\t\t\t\t\t\t<li id="footer-info-lastmod"> This page was last edited on 26 October 2019, at 00:10<span class="anonymous-show">&#160;(UTC)</span>.</li>\n\t\t\t\t\t\t\t\t<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t<ul id="footer-places">\n\t\t\t\t\t\t\t\t<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Multilayer_perceptron&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>\n\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t\t\t\t\t<ul id="footer-icons" class="noprint">\n\t\t\t\t\t\t\t\t\t\t<li id="footer-copyrightico">\n\t\t\t\t\t\t<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t\t<li id="footer-poweredbyico">\n\t\t\t\t\t\t<a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t<div style="clear: both;"></div>\n\t\t</div>\n\t\t\n\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.164","walltime":"0.307","ppvisitednodes":{"value":525,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":28079,"limit":2097152},"templateargumentsize":{"value":728,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":6353,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  157.674      1 -total"," 51.20%   80.722      1 Template:Reflist"," 38.27%   60.346      1 Template:Cite_book"," 26.32%   41.503      1 Template:Machine_learning_bar"," 24.60%   38.794      1 Template:Sidebar_with_collapsible_lists"," 12.75%   20.098      1 Template:Longitem"," 11.05%   17.416      1 Template:Hatnote","  9.83%   15.499      1 Template:Nobold","  7.38%   11.638      1 Template:Slink","  4.30%    6.781      1 Template:Small"]},"scribunto":{"limitreport-timeusage":{"value":"0.050","limit":"10.000"},"limitreport-memusage":{"value":2112416,"limit":52428800}},"cachereport":{"origin":"mw1326","timestamp":"20191026001004","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Multilayer perceptron","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Multilayer_perceptron","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q2991667","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q2991667","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2005-07-19T16:51:53Z","dateModified":"2019-10-26T00:10:04Z","image":"https:\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/f\\/fe\\/Kernel_Machine.svg"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":113,"wgHostname":"mw1269"});});</script>\n</body>\n</html>\n'